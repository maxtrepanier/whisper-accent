{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39d6c13-9e52-4bdc-aa77-a09f090ceba4",
   "metadata": {},
   "source": [
    "# Accent detection project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353d156-c5e1-4232-a1f1-6fc980897efb",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3a960-e1c6-44d0-bd38-07abbe00cf83",
   "metadata": {},
   "source": [
    "This project aims at exploring the capabilities of speech recognition systems to capture subtle features of voice, such as accents.\n",
    "\n",
    "In the first part of this project, we attempt to repurpose the speech recognition system [whisper](https://huggingface.co/openai/whisper-large-v3) to perform accent classification. Specifically, we use the encoder part of whisper to perform feature extraction and apply transfer learning to train an accent classifier.\n",
    "\n",
    "The second part of this project is more ambitious and aims to identify within whisper features corresponding to accents, by implementing dictionary learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de8078e-5099-4a03-b47a-e967285a34e9",
   "metadata": {},
   "source": [
    "### Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded8dd4-f3cf-43d1-b7d6-f739806ddbcb",
   "metadata": {},
   "source": [
    "1. Data processing:\n",
    " - increase the number of samples\n",
    " - use data augmentation to generate more samples\n",
    " - separate audio tracks between train/dev sets to avoid data leakage\n",
    "\n",
    "2. Transfer learning\n",
    " - implement classification layers on top of encoder\n",
    " - integrate into pipeline\n",
    " - preprocess data through encoder\n",
    " - make it work on colab, use GPU\n",
    "\n",
    "3. Model review\n",
    " - test usage of the model\n",
    " - review its architecture, in particular transformer architecture (attention heads, positional encoding ...)\n",
    "\n",
    "\n",
    "More fun:\n",
    " - can we build a model that changes the accent of the speaker?\n",
    " - one approach to this problem is using a supervised learning approach, but there is no dataset. We want instead to use neural style transfer. The idea will be to choose two audio clips. One content, the other style. We calculate the activations in a neural network of both of them in a given layer, then minimise a loss function to find dataclip.\n",
    " - Maybe there is a better approach where we predict denoising?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da9e1f-8d60-484a-bb97-a3f48f6ce633",
   "metadata": {},
   "source": [
    "Note: maybe use dataset:\n",
    "https://huggingface.co/datasets/NathanRoll/commonvoice_train_gender_accent_16k\n",
    "also: https://huggingface.co/WillHeld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd892aa-d685-4f08-966f-c4abe604cf49",
   "metadata": {},
   "source": [
    "## Test: the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10514bbd-dc54-4892-8f4b-e301e41f7bd6",
   "metadata": {},
   "source": [
    "We use whisper-small for these tests. Load the model following the instructions given on HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21381f4e-3316-4186-b093-ca402dfffeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 13:03:06.837441: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 13:03:08.173625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-small\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2816fc0d-2bc8-43e3-989d-802df7affc6b",
   "metadata": {},
   "source": [
    "Check that the model runs using the automatic pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f973827b-0648-44f5-b3db-e8c231399c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, symbolies drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of Up Guards and Atom paintings, and Mason's exquisite idles are as national as a jingle poem. Mr. Birkitt Foster's landscapes smile at one much in the same way that Mr. Karker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says like a shampoo and a Turkish bath, next man\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n",
    "IPython.display.Audio(sample['array'], rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4fb0b-907d-47cd-b65a-7957555c8d26",
   "metadata": {},
   "source": [
    "Reproduce this using a manual pipeline. Since we treat it manually, we only decode the first 30s of the clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55fdb55f-3d74-4e83-9dd6-bde05f2a8875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, symbolies drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all, and can discover\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# config\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-small\"\n",
    "\n",
    "# load model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# load processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "ds = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**inputs, language=\"en\")\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e01333-034f-4601-a382-63916389a4c3",
   "metadata": {},
   "source": [
    "Test with custom data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f91a202-f511-4620-8f52-df456da73a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm quite unscrupulous and not very clever. And still they managed to do great mathematics. So it told the kid that if they can do it, why can't you? And that was certainly what turned me on. I came from England to the United States to study physics. I applied to Cornell University to work with Hans Bethe, who is a famous physicist. But the amazing thing was in the very first week I was there, I met Dick Feynman, who is an absolute\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "data, _ = librosa.load(\"dataset/british/rlaPLvETBug_1.mp3\", sr=16e3)\n",
    "inputs = processor(data,  sampling_rate=16e3, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**inputs, language=\"en\")\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8730b2-acd8-4479-a88e-180166af1d88",
   "metadata": {},
   "source": [
    "### Review of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dccb81c-9860-4555-af78-50924f774662",
   "metadata": {},
   "source": [
    "The input data is a Mel spectrogram, which is of dimension (80, 3000) for 80 channels (frequencies) and 3000 datapoints, i.e. sample each 10ms for 30s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1c15d93-cdb9-4d8d-8029-1499a6d97548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 3000])\n"
     ]
    }
   ],
   "source": [
    "print(input_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "965f9e60-d07e-4e99-a45d-4b63d5e668ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b12668fd-8f68-44f4-9704-d6bd99b4ceed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = model.model.encoder.forward(**inputs).last_hidden_state\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592525a-39c8-4089-84db-09da73a4e2f0",
   "metadata": {},
   "source": [
    "## Generating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3c821-fb94-40ed-8e7f-0dcebfcb4e76",
   "metadata": {},
   "source": [
    "See external python script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4db347-bb76-401d-bfd3-61e09c5ab1ff",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46199ec5-062d-4b0b-86c2-cb69f53cf579",
   "metadata": {},
   "source": [
    "We've constructed a small dataset already, see [...].\n",
    "To load it, we use the load_dataset method with the \"audiofolder\" setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63951d3f-f26f-4bf3-bd4c-1d383c5121ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668a31590f3149bab89e6e0b57e250bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"audiofolder\", data_dir=\"dataset/\", streaming=True)  # use streaming to avoid overloading memory\n",
    "# dataset = dataset['train'].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8f78a08-2b18-45f0-b9e5-6e2203975746",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Subclasses of Dataset should implement __getitem__.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14838/1745912358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'audio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT_co\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Subclasses of Dataset should implement __getitem__.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# def __getitems__(self, indices: List) -> List[T_co]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Subclasses of Dataset should implement __getitem__."
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "print(dataset['train'][0])\n",
    "IPython.display.Audio(dataset['train'][0]['audio']['array'], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0421dc1c-f945-429c-8c7c-c7fa3df679fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(item):\n",
    "    return processor(item['audio']['array'], sampling_rate=16e3, return_tensors=\"pt\")\n",
    "dataset_preprocessed = dataset['train'].map(preprocess_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ed80fb8-d307-45ff-a05b-16aa4405b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/maxime/Dropbox/Job preparation/Machine learning/Speech/dataset/american/0ti98bgCNT0_0.mp3', 'array': array([ 0.10973359,  0.19582772,  0.22934687, ...,  0.00101544,\n",
      "       -0.00177654,  0.00240936]), 'sampling_rate': 16000}, 'label': 0, 'input_features': tensor([[[ 0.6304,  0.9559,  0.9835,  ...,  0.2060,  0.1273,  0.2613],\n",
      "         [ 0.8856,  0.9671,  1.0197,  ...,  0.6224,  0.5875,  0.5247],\n",
      "         [ 0.9071,  0.9066,  0.8470,  ...,  0.6629,  0.6388,  0.5920],\n",
      "         ...,\n",
      "         [ 0.3085,  0.3428,  0.4108,  ...,  0.0290, -0.1313, -0.1456],\n",
      "         [ 0.1618, -0.0891,  0.1418,  ..., -0.4652, -0.4652, -0.4436],\n",
      "         [ 0.1456, -0.3655, -0.4652,  ..., -0.4652, -0.4652, -0.4652]]])}\n"
     ]
    }
   ],
   "source": [
    "for item in dataset_preprocessed:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6089d811-2bfb-4a80-b443-74732ff526c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method map in module datasets.arrow_dataset:\n",
      "\n",
      "map(function: Optional[Callable] = None, with_indices: bool = False, with_rank: bool = False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Optional[int] = 1000, drop_last_batch: bool = False, remove_columns: Union[str, List[str], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, features: Optional[datasets.features.features.Features] = None, disable_nullable: bool = False, fn_kwargs: Optional[dict] = None, num_proc: Optional[int] = None, suffix_template: str = '_{rank:05d}_of_{num_proc:05d}', new_fingerprint: Optional[str] = None, desc: Optional[str] = None) -> 'Dataset' method of datasets.arrow_dataset.Dataset instance\n",
      "    Apply a function to all the examples in the table (individually or in batches) and update the table.\n",
      "    If your function returns a column that already exists, then it overwrites it.\n",
      "    \n",
      "    You can specify whether the function should be batched or not with the `batched` parameter:\n",
      "    \n",
      "    - If batched is `False`, then the function takes 1 example in and should return 1 example.\n",
      "      An example is a dictionary, e.g. `{\"text\": \"Hello there !\"}`.\n",
      "    - If batched is `True` and `batch_size` is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples.\n",
      "      A batch is a dictionary, e.g. a batch of 1 example is `{\"text\": [\"Hello there !\"]}`.\n",
      "    - If batched is `True` and `batch_size` is `n > 1`, then the function takes a batch of `n` examples as input and can return a batch with `n` examples, or with an arbitrary number of examples.\n",
      "      Note that the last batch may have less than `n` examples.\n",
      "      A batch is a dictionary, e.g. a batch of `n` examples is `{\"text\": [\"Hello there !\"] * n}`.\n",
      "    \n",
      "    Args:\n",
      "        function (`Callable`): Function with one of the following signatures:\n",
      "    \n",
      "            - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False` and `with_rank=False`\n",
      "            - `function(example: Dict[str, Any], *extra_args) -> Dict[str, Any]` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
      "            - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False` and `with_rank=False`\n",
      "            - `function(batch: Dict[str, List], *extra_args) -> Dict[str, List]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
      "    \n",
      "            For advanced usage, the function can also return a `pyarrow.Table`.\n",
      "            Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n",
      "            If no function is provided, default to identity function: `lambda x: x`.\n",
      "        with_indices (`bool`, defaults to `False`):\n",
      "            Provide example indices to `function`. Note that in this case the\n",
      "            signature of `function` should be `def function(example, idx[, rank]): ...`.\n",
      "        with_rank (`bool`, defaults to `False`):\n",
      "            Provide process rank to `function`. Note that in this case the\n",
      "            signature of `function` should be `def function(example[, idx], rank): ...`.\n",
      "        input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`):\n",
      "            The columns to be passed into `function`\n",
      "            as positional arguments. If `None`, a `dict` mapping to all formatted columns is passed as one argument.\n",
      "        batched (`bool`, defaults to `False`):\n",
      "            Provide batch of examples to `function`.\n",
      "        batch_size (`int`, *optional*, defaults to `1000`):\n",
      "            Number of examples per batch provided to `function` if `batched=True`.\n",
      "            If `batch_size <= 0` or `batch_size == None`, provide the full dataset as a single batch to `function`.\n",
      "        drop_last_batch (`bool`, defaults to `False`):\n",
      "            Whether a last batch smaller than the batch_size should be\n",
      "            dropped instead of being processed by the function.\n",
      "        remove_columns (`Optional[Union[str, List[str]]]`, defaults to `None`):\n",
      "            Remove a selection of columns while doing the mapping.\n",
      "            Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      "            columns with names in `remove_columns`, these columns will be kept.\n",
      "        keep_in_memory (`bool`, defaults to `False`):\n",
      "            Keep the dataset in memory instead of writing it to a cache file.\n",
      "        load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
      "            If a cache file storing the current computation from `function`\n",
      "            can be identified, use it instead of recomputing.\n",
      "        cache_file_name (`str`, *optional*, defaults to `None`):\n",
      "            Provide the name of a path for the cache file. It is used to store the\n",
      "            results of the computation instead of the automatically generated cache file name.\n",
      "        writer_batch_size (`int`, defaults to `1000`):\n",
      "            Number of rows per write operation for the cache file writer.\n",
      "            This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      "            Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      "        features (`Optional[datasets.Features]`, defaults to `None`):\n",
      "            Use a specific Features to store the cache file\n",
      "            instead of the automatically generated one.\n",
      "        disable_nullable (`bool`, defaults to `False`):\n",
      "            Disallow null values in the table.\n",
      "        fn_kwargs (`Dict`, *optional*, defaults to `None`):\n",
      "            Keyword arguments to be passed to `function`.\n",
      "        num_proc (`int`, *optional*, defaults to `None`):\n",
      "            Max number of processes when generating cache. Already cached shards are loaded sequentially.\n",
      "        suffix_template (`str`):\n",
      "            If `cache_file_name` is specified, then this suffix\n",
      "            will be added at the end of the base name of each. Defaults to `\"_{rank:05d}_of_{num_proc:05d}\"`. For example, if `cache_file_name` is \"processed.arrow\", then for\n",
      "            `rank=1` and `num_proc=4`, the resulting file would be `\"processed_00001_of_00004.arrow\"` for the default suffix.\n",
      "        new_fingerprint (`str`, *optional*, defaults to `None`):\n",
      "            The new fingerprint of the dataset after transform.\n",
      "            If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "        desc (`str`, *optional*, defaults to `None`):\n",
      "            Meaningful description to be displayed alongside with the progress bar while mapping examples.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      "    >>> def add_prefix(example):\n",
      "    ...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
      "    ...     return example\n",
      "    >>> ds = ds.map(add_prefix)\n",
      "    >>> ds[0:3][\"text\"]\n",
      "    ['Review: compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .',\n",
      "     'Review: the soundtrack alone is worth the price of admission .',\n",
      "     'Review: rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .']\n",
      "    \n",
      "    # process a batch of examples\n",
      "    >>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n",
      "    # set number of processors\n",
      "    >>> ds = ds.map(add_prefix, num_proc=4)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dataset['train'].map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "593f76cf-df0c-46b2-bbbd-141d2a6c5547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 3000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(dataset_preprocessed[0]['input_features']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8056ed66-ad9c-4f52-b607-bd32e089685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset_preprocessed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6375a-18b4-4bd4-a772-49a6d53567f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(audio_files, whisper_model):\n",
    "    preprocessed_data = []\n",
    "    for audio_file in audio_files:\n",
    "        # Load and preprocess audio (implement this based on your audio format)\n",
    "        audio_input = load_and_preprocess_audio(audio_file)\n",
    "        \n",
    "        # Run through Whisper encoder\n",
    "        with torch.no_grad():\n",
    "            encoder_output = whisper_model.encoder(audio_input).last_hidden_state\n",
    "        \n",
    "        # Store the output\n",
    "        preprocessed_data.append(encoder_output.cpu().numpy())\n",
    "    \n",
    "    return np.array(preprocessed_data)\n",
    "\n",
    "# Usage\n",
    "whisper_model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "preprocessed_data = preprocess_dataset(your_audio_files, whisper_model)\n",
    "np.save('preprocessed_whisper_features.npy', preprocessed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2163f3-308e-4b20-8839-e943b2a337aa",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe192b6-dad1-4610-8f93-3552183ee758",
   "metadata": {},
   "source": [
    "We start by implementing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "537924d3-7974-4a0a-ada9-37c5b2bdfc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# class generated with Claude\n",
    "\n",
    "class AccentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=2, dropout_rate=0.5):\n",
    "        super(AccentClassifier, self).__init__()\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 1500, 768) (output of Whisper encoder)\n",
    "        \n",
    "        # Permute dimensions for pooling\n",
    "        x = x.permute(0, 2, 1)  # shape: (batch_size, 768, 1500)\n",
    "        \n",
    "        # Apply average pooling\n",
    "        x = self.avg_pool(x)  # shape: (batch_size, 768, 1)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # shape: (batch_size, 768)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "# whisper_output = torch.randn(32, 1500, 768)  # Example batch of Whisper encoder outputs\n",
    "# model = AccentClassifier()\n",
    "# output = model(whisper_output)\n",
    "# print(output.shape)  # Should be torch.Size([32, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3c39eea-5f56-412b-b691-a79c7fbc2591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "class AccentModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, use_encoder=True):\n",
    "        super(AccentModel, self).__init__()\n",
    "\n",
    "        self.use_encoder = use_encoder\n",
    "\n",
    "        # config\n",
    "        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        model_id = \"openai/whisper-small\"\n",
    "\n",
    "        # load model\n",
    "        self.whisper_encoder = WhisperForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "           ).model.encoder\n",
    "        \n",
    "        # Freeze Whisper encoder weights\n",
    "        for param in self.whisper_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.accent_classifier = AccentClassifier(num_classes=num_classes)\n",
    "\n",
    "    def encode(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.whisper_encoder(x).last_hidden_state\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_encoder:\n",
    "            encoder_output = self.encode(x)\n",
    "        else:\n",
    "            encoder_output = x\n",
    "        return self.accent_classifier(encoder_output)\n",
    "\n",
    "# Example usage:\n",
    "# model = FullAccentModel()\n",
    "# input_features = torch.randn(32, 80, 3000)  # Example input to Whisper\n",
    "# output = model(input_features)\n",
    "# print(output.shape)  # Should be torch.Size([32, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504a472-cdb2-4edb-8ee7-35e26f9bc91b",
   "metadata": {},
   "source": [
    "Instantiation and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f2d1a12-feb1-4198-bc89-a7c55d120bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1335, -1.4753]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AccentModel(use_encoder=False)\n",
    "model.to(device)\n",
    "\n",
    "x = torch.rand((1,1500,768))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee00e52-01bd-4f2f-bf35-31d5ea781a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
