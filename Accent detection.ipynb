{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39d6c13-9e52-4bdc-aa77-a09f090ceba4",
   "metadata": {},
   "source": [
    "# Accent detection project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353d156-c5e1-4232-a1f1-6fc980897efb",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3a960-e1c6-44d0-bd38-07abbe00cf83",
   "metadata": {},
   "source": [
    "This project aims at exploring the capabilities of speech recognition systems to capture subtle features of voice, such as accents.\n",
    "\n",
    "In the first part of this project, we attempt to repurpose the speech recognition system [whisper](https://huggingface.co/openai/whisper-large-v3) to perform accent classification. Specifically, we use the encoder part of whisper to perform feature extraction and apply transfer learning to train an accent classifier.\n",
    "\n",
    "The second part of this project is more ambitious and aims to identify within whisper features corresponding to accents, by implementing dictionary learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de8078e-5099-4a03-b47a-e967285a34e9",
   "metadata": {},
   "source": [
    "### Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded8dd4-f3cf-43d1-b7d6-f739806ddbcb",
   "metadata": {},
   "source": [
    "1. Data processing:\n",
    " - increase the number of samples\n",
    " - use data augmentation to generate more samples\n",
    " - separate audio tracks between train/dev sets to avoid data leakage\n",
    "\n",
    "2. Transfer learning\n",
    " - implement classification layers on top of encoder\n",
    " - integrate into pipeline\n",
    " - preprocess data through encoder\n",
    " - make it work on colab, use GPU\n",
    "\n",
    "3. Model review\n",
    " - test usage of the model\n",
    " - review its architecture, in particular transformer architecture (attention heads, positional encoding ...)\n",
    "\n",
    "\n",
    "More fun:\n",
    " - can we build a model that changes the accent of the speaker?\n",
    " - one approach to this problem is using a supervised learning approach, but there is no dataset. We want instead to use neural style transfer. The idea will be to choose two audio clips. One content, the other style. We calculate the activations in a neural network of both of them in a given layer, then minimise a loss function to find dataclip.\n",
    " - Maybe there is a better approach where we predict denoising?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da9e1f-8d60-484a-bb97-a3f48f6ce633",
   "metadata": {},
   "source": [
    "Note: maybe use dataset:\n",
    "https://huggingface.co/datasets/NathanRoll/commonvoice_train_gender_accent_16k\n",
    "also: https://huggingface.co/WillHeld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd892aa-d685-4f08-966f-c4abe604cf49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test: the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10514bbd-dc54-4892-8f4b-e301e41f7bd6",
   "metadata": {},
   "source": [
    "We use whisper-small for these tests. Load the model following the instructions given on HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21381f4e-3316-4186-b093-ca402dfffeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 13:03:06.837441: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 13:03:08.173625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-small\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2816fc0d-2bc8-43e3-989d-802df7affc6b",
   "metadata": {},
   "source": [
    "Check that the model runs using the automatic pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f973827b-0648-44f5-b3db-e8c231399c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, symbolies drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of Up Guards and Atom paintings, and Mason's exquisite idles are as national as a jingle poem. Mr. Birkitt Foster's landscapes smile at one much in the same way that Mr. Karker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says like a shampoo and a Turkish bath, next man\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n",
    "IPython.display.Audio(sample['array'], rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4fb0b-907d-47cd-b65a-7957555c8d26",
   "metadata": {},
   "source": [
    "Reproduce this using a manual pipeline. Since we treat it manually, we only decode the first 30s of the clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55fdb55f-3d74-4e83-9dd6-bde05f2a8875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, symbolies drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all, and can discover\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# config\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-small\"\n",
    "\n",
    "# load model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# load processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "ds = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**inputs, language=\"en\")\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e01333-034f-4601-a382-63916389a4c3",
   "metadata": {},
   "source": [
    "Test with custom data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f91a202-f511-4620-8f52-df456da73a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm quite unscrupulous and not very clever. And still they managed to do great mathematics. So it told the kid that if they can do it, why can't you? And that was certainly what turned me on. I came from England to the United States to study physics. I applied to Cornell University to work with Hans Bethe, who is a famous physicist. But the amazing thing was in the very first week I was there, I met Dick Feynman, who is an absolute\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "data, _ = librosa.load(\"dataset/british/rlaPLvETBug_1.mp3\", sr=16e3)\n",
    "inputs = processor(data,  sampling_rate=16e3, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**inputs, language=\"en\")\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8730b2-acd8-4479-a88e-180166af1d88",
   "metadata": {},
   "source": [
    "### Review of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dccb81c-9860-4555-af78-50924f774662",
   "metadata": {},
   "source": [
    "The input data is a Mel spectrogram, which is of dimension (80, 3000) for 80 channels (frequencies) and 3000 datapoints, i.e. sample each 10ms for 30s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1c15d93-cdb9-4d8d-8029-1499a6d97548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 3000])\n"
     ]
    }
   ],
   "source": [
    "print(input_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "965f9e60-d07e-4e99-a45d-4b63d5e668ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b12668fd-8f68-44f4-9704-d6bd99b4ceed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = model.model.encoder.forward(**inputs).last_hidden_state\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592525a-39c8-4089-84db-09da73a4e2f0",
   "metadata": {},
   "source": [
    "## Generating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3c821-fb94-40ed-8e7f-0dcebfcb4e76",
   "metadata": {},
   "source": [
    "See external python script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4db347-bb76-401d-bfd3-61e09c5ab1ff",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46199ec5-062d-4b0b-86c2-cb69f53cf579",
   "metadata": {},
   "source": [
    "We've constructed a small dataset already, see [...].\n",
    "To load it, we use the load_dataset method with the \"audiofolder\" setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63951d3f-f26f-4bf3-bd4c-1d383c5121ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 14:04:06.568437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-26 14:04:08.059217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f34e645e21d40e5bdc2ed7800d6a93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d7568225114c0cb66cdc626739b254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import IPython\n",
    "import whisper_accent\n",
    "\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"dataset/\", streaming=True, )  # use streaming to avoid overloading memory\n",
    "# dataset = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "model_id = \"openai/whisper-small\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "whisper_model = whisper_accent.AccentModel(use_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f78a08-2b18-45f0-b9e5-6e2203975746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/maxime/Dropbox/Job preparation/Machine learning/Speech/dataset/american/train/-96-lAfagow_0.mp3', 'array': array([-0.11506341, -0.12139536, -0.10677178, ..., -0.00088448,\n",
      "       -0.00142263, -0.00095254]), 'sampling_rate': 16000}, 'label': None}\n"
     ]
    }
   ],
   "source": [
    "streaming = True\n",
    "if streaming:\n",
    "    item = next(iter(dataset['train']))\n",
    "    print(item)\n",
    "else:\n",
    "    print(dataset['train'][0])\n",
    "    IPython.display.Audio(dataset['train'][0]['audio']['array'], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d1db2-bab3-47b0-837a-84df96e6b5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f634d502-ecc7-40af-a537-cd2b50052489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None),\n",
       " 'label': ClassLabel(names=['dev', 'train'], id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0421dc1c-f945-429c-8c7c-c7fa3df679fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_file, whisper_model):\n",
    "    # Load and preprocess audio (implement this based on your audio format)\n",
    "    audio_input = processor(audio_file['audio']['array'], sampling_rate=16e3, return_tensors=\"pt\")\n",
    "    \n",
    "    # Run through Whisper encoder\n",
    "    encoder_output = whisper_model.encode(audio_input.input_features)\n",
    "    \n",
    "    return {'input_features': encoder_output}\n",
    "\n",
    "def preprocess_dataset(audio_files, whisper_model):\n",
    "    preprocessed_data = []\n",
    "    for audio_file in audio_files:\n",
    "        # Load and preprocess audio (implement this based on your audio format)\n",
    "        audio_input = processor(audio_file['audio']['array'], sampling_rate=16e3, return_tensors=\"pt\")\n",
    "        \n",
    "        # Run through Whisper encoder\n",
    "        encoder_output = whisper_model.encode(audio_input.input_features)\n",
    "        preprocessed_data.append(encoder_output.cpu().numpy())\n",
    "    \n",
    "    return np.array(preprocessed_data)\n",
    "\n",
    "# Usage\n",
    "# whisper_model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "# preprocessed_data = preprocess_dataset(your_audio_files, whisper_model)\n",
    "# np.save('preprocessed_whisper_features.npy', preprocessed_data)\n",
    "        \n",
    "# Usage\n",
    "dataset_preprocessed = dataset['train'].map(lambda sample: preprocess_audio(sample, whisper_model))\n",
    "loader = DataLoader(dataset_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ed80fb8-d307-45ff-a05b-16aa4405b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/maxime/Dropbox/Job preparation/Machine learning/Speech/dataset/american/train/-96-lAfagow_0.mp3', 'array': array([-0.11506341, -0.12139536, -0.10677178, ..., -0.00088448,\n",
      "       -0.00142263, -0.00095254]), 'sampling_rate': 16000}, 'label': None, 'input_features': tensor([[[-1.2333, -1.3540,  2.3076,  ..., -0.0070, -2.3963, -0.3603],\n",
      "         [ 0.8706,  1.1474,  3.8535,  ...,  1.3523, -2.6281,  0.9858],\n",
      "         [-0.3164,  1.1053,  3.5141,  ...,  1.5767, -1.3818,  1.1494],\n",
      "         ...,\n",
      "         [-0.3893, -0.9009,  1.2607,  ..., -0.8640, -0.1840, -0.7558],\n",
      "         [-0.6525,  0.0466,  1.2445,  ..., -0.8179, -0.6968, -0.8501],\n",
      "         [-1.9222,  0.0985,  1.4866,  ..., -1.3926, -0.6052, -0.1313]]])}\n",
      "tensor([[-0.0853,  0.0628]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset_preprocessed:\n",
    "    print(whisper_model.forward(item['input_features']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8056ed66-ad9c-4f52-b607-bd32e089685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset_preprocessed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6375a-18b4-4bd4-a772-49a6d53567f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(audio_files, whisper_model):\n",
    "    preprocessed_data = []\n",
    "    for audio_file in audio_files:\n",
    "        # Load and preprocess audio (implement this based on your audio format)\n",
    "        audio_input = load_and_preprocess_audio(audio_file)\n",
    "        \n",
    "        # Run through Whisper encoder\n",
    "        with torch.no_grad():\n",
    "            encoder_output = whisper_model.encoder(audio_input).last_hidden_state\n",
    "        \n",
    "        # Store the output\n",
    "        preprocessed_data.append(encoder_output.cpu().numpy())\n",
    "    \n",
    "    return np.array(preprocessed_data)\n",
    "\n",
    "# Usage\n",
    "whisper_model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "preprocessed_data = preprocess_dataset(your_audio_files, whisper_model)\n",
    "np.save('preprocessed_whisper_features.npy', preprocessed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2163f3-308e-4b20-8839-e943b2a337aa",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe192b6-dad1-4610-8f93-3552183ee758",
   "metadata": {},
   "source": [
    "We start by implementing the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504a472-cdb2-4edb-8ee7-35e26f9bc91b",
   "metadata": {},
   "source": [
    "Instantiation and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f2d1a12-feb1-4198-bc89-a7c55d120bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1335, -1.4753]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AccentModel(use_encoder=False)\n",
    "model.to(device)\n",
    "\n",
    "x = torch.rand((1,1500,768))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee00e52-01bd-4f2f-bf35-31d5ea781a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
